import gym
import time
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.optimizers.legacy import Adam
from rl.agents.dqn import DQNAgent
from rl.policy import BoltzmannQPolicy
from rl.memory import SequentialMemory
import os


####
####


os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'

env = gym.make("LunarLander-v2", render_mode=None)
#env.reset()

#env.render()  # Enable rendering

states = env.observation_space.shape[0]
actions = env.action_space.n

print(states, actions)

model = Sequential()
model.add(Flatten(input_shape=(1, states)))
model.add(Dense(24, activation="relu"))
model.add(Dense(24, activation="relu"))
model.add(Dense(actions, activation="linear"))

agent = DQNAgent(
    model=model,
    memory=SequentialMemory(limit=50000, window_length=1),
    policy=BoltzmannQPolicy(),
    nb_actions=actions,
    nb_steps_warmup=10,
    target_model_update=0.01
)

optimizer = Adam(learning_rate=0.001)
agent.compile(optimizer, metrics=["mae"])
agent.fit(env, nb_steps=100000, visualize=False, verbose=1)

results = agent.test(env, nb_episodes=10, visualize=True)

print(np.mean(results.history["episode_reward"]))

env.close()

# Episodes with rendering
episodes = 10

for episode in range(1, episodes + 1):
    state = env.reset()  # Reset the environment
    done = False
    score = 0

    while not done:
        action = agent.policy(state)  # Use the agent's policy to get the action
        next_state, reward, done, _ = env.step(action)
        score += reward
        #env.render()
        state = next_state

    print(f"Episode {episode}, Score: {score}")
    time.sleep(1)

env.close()
