import random
import gym
import time

import numpy as np

#from pyvirtualdisplay import Display

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten 
from tensorflow.keras.optimizers.legacy  import Adam
from tensorflow import keras
from rl.agents.dqn import DQNAgent 
from rl.policy  import BoltzmannQPolicy
from rl.memory import SequentialMemory


import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1' 


#display = Display(visible=0, size=(1400, 900))
##display.start()

env = gym.make("CartPole-v1")

#env.render(mode="human")

states = env.observation_space.shape[0]
actions = env.action_space.n


print(states, actions)


model = Sequential()
model.add(Flatten(input_shape=(1, states)))
model.add(Dense(24, activation="relu"))
model.add(Dense(24, activation="relu"))
model.add(Dense(actions, activation="linear"))

agent = DQNAgent(
    model=model,
    memory=SequentialMemory(limit=50000, window_length=1),
    policy=BoltzmannQPolicy(),
    nb_actions=actions,
    nb_steps_warmup=10,
    target_model_update=0.01
)

optimizer = Adam(learning_rate=0.001)
agent.compile(optimizer, metrics=["mae"])
agent.fit(env, nb_steps=100000, visualize=False, verbose=1)


results = agent.test(env, nb_episodes=10, visualize=True)


print(np.mean(results.history["episode_reward"]))

env.close()
"""
episodes = 10

for episode in range (1, episodes+1):
    state = env.reset()
    done = False
    score = 0

    while True:
        action = random.choice([0, 1])
        _, reward, done, _ = env.step(action)
        score += reward
        env.render()
        time.sleep(1)

    print(f"Episode {episode}, Score: {score}")
    time.sleep(1)

#display.stop()
env.close()
"""